{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30602,"status":"ok","timestamp":1708278297701,"user":{"displayName":"Mansi Mishra","userId":"04256905725587643817"},"user_tz":-330},"id":"sIRzIkgFLzlD","outputId":"903785f7-066c-4ea4-b7ac-584fbdb7b2b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/AWS_AI_ML_Scholar\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd \"/content/drive/MyDrive/AWS_AI_ML_Scholar\""]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35815,"status":"ok","timestamp":1708278267110,"user":{"displayName":"Mansi Mishra","userId":"04256905725587643817"},"user_tz":-330},"id":"oMPb5pdzL8qx","outputId":"c8db35d8-815b-4c59-dfd5-80488e003f4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Directory created: ./flower_data\n","\n","[INFO] Downloading the file 'flower_data.tar.gz' to ./flower_data\n","[INFO] 'flower_data.tar.gz' saved to ./flower_data\n","\n","[INFO] Extracting the downloaded tarball to ./flower_data\n","[INFO] 'flower_data.tar.gz' extracted successfully to ./flower_data\n","\n","[INFO] Deleting the tarball to save space.\n"]}],"source":["# imports\n","import os\n","import requests\n","from pathlib import Path\n","import tarfile\n","\n","# defining dataset directory\n","data_dir = './flower_data'\n","\n","# using pathlib.Path for handling PosixPath\n","FLOWERS_DIR = Path(data_dir)\n","\n","# downloading and setting up data if not already present\n","if not FLOWERS_DIR.is_dir():\n","    # creating directory\n","    FLOWERS_DIR.mkdir(parents=True, exist_ok=True)\n","    print(f\"[INFO] Directory created: ./{FLOWERS_DIR}\")\n","\n","    print() # for readability\n","\n","    # tarball path\n","    TARBALL = FLOWERS_DIR / \"flower_data.tar.gz\"\n","\n","    # downloading and writing the tarball to './flowers' directory\n","    print(f\"[INFO] Downloading the file 'flower_data.tar.gz' to ./{FLOWERS_DIR}\")\n","    request = requests.get('https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz')\n","    with open(TARBALL, \"wb\") as file_ref:\n","        file_ref.write(request.content)\n","        print(f\"[INFO] 'flower_data.tar.gz' saved to ./{FLOWERS_DIR}\")\n","\n","    print() # for readability\n","\n","    # extracting the downloaded tarball\n","    print(f\"[INFO] Extracting the downloaded tarball to ./{FLOWERS_DIR}\")\n","    with tarfile.open(TARBALL, \"r\") as tar_ref:\n","        tar_ref.extractall(FLOWERS_DIR)\n","        print(f\"[INFO] 'flower_data.tar.gz' extracted successfully to ./{FLOWERS_DIR}\")\n","\n","    print() # for readability\n","\n","    # using os.remove to delete the downloaded tarball\n","    print(\"[INFO] Deleting the tarball to save space.\")\n","    os.remove(TARBALL)\n","else:\n","    print(f\"[INFO] Dataset already setup at ./{FLOWERS_DIR}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2y-gb6YOMKdD","executionInfo":{"status":"ok","timestamp":1708278313066,"user_tz":-330,"elapsed":917,"user":{"displayName":"Mansi Mishra","userId":"04256905725587643817"}}},"outputs":[],"source":["import json\n","\n","data = {\n","    \"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\",\n","    \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\",\n","    \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\",\n","    \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\",\n","    \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\",\n","    \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\",\n","    \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\",\n","    \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\",\n","    \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\",\n","    \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\",\n","    \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\",\n","    \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\",\n","    \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\",\n","    \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\",\n","    \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\",\n","    \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\",\n","    \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\",\n","    \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\",\n","    \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\",\n","    \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\",\n","    \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"\n","}\n","\n","with open('cat_to_name.json', 'w') as file:\n","    json.dump(data, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBq0He-_MQ4e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92a34caf-53fb-4ffa-873f-0ae2d542d3cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of classes in the dataset: 102\n","Number of training samples: 6552\n","Number of testing samples: 819\n","Number of validation samples: 818\n","Training started...\n"]}],"source":["import torch\n","import argparse\n","import torchvision.transforms as transforms\n","from torch import nn, optim\n","from torchvision import datasets, models\n","from torch.utils.data import DataLoader\n","from collections import OrderedDict\n","from PIL import Image\n","\n","# Define transformation functions for the datasets\n","def data_transformations(train_dir, test_dir, valid_dir):\n","    # Define transformations for training data\n","    train_transforms = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(30),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","    # Define transformations for testing and validation data\n","    test_valid_transforms = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","    # Load datasets with specified transformations\n","    train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n","    test_data = datasets.ImageFolder(test_dir, transform=test_valid_transforms)\n","    valid_data = datasets.ImageFolder(valid_dir, transform=test_valid_transforms)\n","\n","    # Print the number of classes in the dataset for validation\n","    num_classes = len(train_data.classes)\n","    print(f\"Number of classes in the dataset: {num_classes}\")\n","\n","    # Calculate and print the number of samples in each subset\n","    num_train_samples = len(train_data)\n","    num_test_samples = len(test_data)\n","    num_valid_samples = len(valid_data)\n","    print(f\"Number of training samples: {num_train_samples}\")\n","    print(f\"Number of testing samples: {num_test_samples}\")\n","    print(f\"Number of validation samples: {num_valid_samples}\")\n","\n","    return train_data, test_data, valid_data\n","\n","\n","\n","# Define classifier architecture\n","def classifier_definition(input_size, hidden_units):\n","    \"\"\"\n","    Define the classifier architecture.\n","\n","    Args:\n","        input_size (int): The size of the input features.\n","        hidden_units (int): The number of units in the hidden layer.\n","\n","    Returns:\n","        nn.Sequential: The classifier model.\n","    \"\"\"\n","\n","    # Define the structure of the classifier\n","    classifier = nn.Sequential(OrderedDict([\n","        ('fc1', nn.Linear(input_size, hidden_units)),\n","        ('relu1', nn.ReLU()),\n","        ('dropout1', nn.Dropout(0.5)),\n","        ('fc2', nn.Linear(hidden_units, 102)),  # Output layer with 102 units for 102 classes\n","        ('output', nn.LogSoftmax(dim=1))\n","    ]))\n","\n","    return classifier  # Return the classifier model\n","\n","\n","# Initialize pre-trained model\n","def model_initialization(architecture):\n","    \"\"\"\n","    Initialize a pre-trained model based on the specified architecture.\n","\n","    Args:\n","        architecture (str): The name of the architecture to use.\n","\n","    Returns:\n","        tuple: A tuple containing the initialized model and its input size.\n","    \"\"\"\n","\n","    # Check the specified architecture and initialize the corresponding pre-trained model\n","    if architecture == 'vgg19':\n","        model = models.vgg19(pretrained=True)\n","        input_size = 25088\n","    elif architecture == 'densenet121':\n","        model = models.densenet121(pretrained=True)\n","        input_size = 1024\n","    elif architecture == 'alexnet':\n","        model = models.alexnet(pretrained=True)\n","        input_size = 9216\n","    else:\n","        # Raise a ValueError if an invalid model architecture is provided\n","        raise ValueError(\"Invalid model architecture\")\n","\n","    # Freeze the parameters of the pre-trained model\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Return a tuple containing the initialized model and its input size\n","    return model, input_size\n","\n","\n","# Define data loaders\n","def create_loaders(train_data, test_data, valid_data):\n","    \"\"\"\n","    Create data loaders for the training, testing, and validation datasets.\n","\n","    Args:\n","        train_data (Dataset): Training dataset.\n","        test_data (Dataset): Testing dataset.\n","        valid_data (Dataset): Validation dataset.\n","\n","    Returns:\n","        tuple: A tuple containing the data loaders for training, testing, and validation datasets.\n","    \"\"\"\n","    # Create data loaders for the training, testing, and validation datasets\n","    valid_loader = DataLoader(valid_data, batch_size=64, shuffle=False)\n","    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","\n","    # Return a tuple containing the data loaders for training, testing, and validation datasets\n","    return train_loader, test_loader, valid_loader\n","\n","\n","# Train the model\n","def model_training(model, train_loader, valid_loader, criterion, optimizer, device, epochs):\n","    \"\"\"\n","    Train the model using the provided data loaders.\n","\n","    Args:\n","        model (nn.Module): The model to be trained.\n","        train_loader (DataLoader): Data loader for the training dataset.\n","        valid_loader (DataLoader): Data loader for the validation dataset.\n","        criterion (nn.Module): Loss function criterion.\n","        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n","        device (torch.device): Device to be used for training.\n","        epochs (int): Number of epochs for training.\n","    \"\"\"\n","    print(\"Training started...\")\n","    model.to(device)\n","    steps = 0\n","    print_every = 40\n","    running_loss = 0\n","\n","    for epoch in range(epochs):\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            logps = model.forward(inputs)\n","            loss = criterion(logps, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            steps += 1\n","\n","            if steps % print_every == 0:\n","                validation_loss = 0\n","                accuracy = 0\n","                model.eval()\n","\n","                with torch.no_grad():\n","                    for inputs, labels in valid_loader:\n","                        inputs, labels = inputs.to(device), labels.to(device)\n","                        logps = model.forward(inputs)\n","                        batch_loss = criterion(logps, labels)\n","                        validation_loss += batch_loss.item()\n","                        ps = torch.exp(logps)\n","                        top_p, top_class = ps.topk(1, dim=1)\n","                        equals = top_class == labels.view(*top_class.shape)\n","                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","                print(f\"Epoch {epoch + 1}/{epochs}.. \"\n","                      f\"Training loss: {running_loss/print_every:.3f}.. \"\n","                      f\"Validation loss: {validation_loss/len(valid_loader):.3f}.. \"\n","                      f\"Validation accuracy: {accuracy/len(valid_loader):.3f}\")\n","                running_loss = 0\n","                model.train()\n","\n","# Test the trained model\n","def model_testing(model, test_loader, device):\n","    \"\"\"\n","    Test the trained model using the provided test data loader.\n","\n","    Args:\n","        model (nn.Module): The trained model to be tested.\n","        test_loader (DataLoader): Data loader for the test dataset.\n","        device (torch.device): Device to be used for testing.\n","    \"\"\"\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f\"Accuracy on test images: {accuracy:.2f}%\")\n","\n","# Save the trained model checkpoint\n","def save_model_checkpoint(model, optimizer, input_size, output_size, epochs, save_dir):\n","    \"\"\"\n","    Save the trained model checkpoint to the specified directory.\n","\n","    Args:\n","        model (nn.Module): The trained model to be saved.\n","        optimizer (torch.optim.Optimizer): Optimizer used during training.\n","        input_size (int): Size of the input layer.\n","        output_size (int): Size of the output layer.\n","        epochs (int): Number of epochs trained for.\n","        save_dir (str): Directory path to save the checkpoint.\n","    \"\"\"\n","    model.class_to_idx = train_data.class_to_idx\n","    checkpoint = {\n","        'input_size': input_size,\n","        'output_size': output_size,\n","        'hidden_units': args.hidden_units,\n","        'state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'class_to_idx': model.class_to_idx,\n","        'epochs': epochs,\n","        'arch': args.arch\n","    }\n","    torch.save(checkpoint, save_dir)\n","    print(f\"Model checkpoint saved to {save_dir}\")\n","\n","\n","# Main function\n","if __name__ == \"__main__\":\n","    # Parse command line arguments\n","    parser = argparse.ArgumentParser(description=\"Train a neural network on a custom dataset.\")\n","    parser.add_argument('--data_dir', help='Path to dataset directory', default=\"flower_data\", type=str)\n","    parser.add_argument('--hidden_units', type=int, dest=\"hidden_units\", action=\"store\", default=120)\n","    parser.add_argument('--gpu', dest=\"gpu\", action=\"store\", default=\"gpu\")\n","    parser.add_argument('--epochs', dest=\"epochs\", action=\"store\", type=int, default=10)\n","    parser.add_argument('--arch', dest=\"arch\", action=\"store\", default=\"vgg19\", type=str, choices=['vgg19', 'densenet121', 'alexnet'])\n","    parser.add_argument('--save_dir', dest=\"save_dir\", action=\"store\", default=\"./checkpoint.pth\")\n","    parser.add_argument('--learning_rate', dest=\"learning_rate\", action=\"store\", default=0.001, type=float)\n","    args = parser.parse_args(\"\")\n","\n","    train_dir = args.data_dir + '/train'\n","    test_dir = args.data_dir + '/test'\n","    valid_dir = args.data_dir + '/valid'\n","\n","    train_data, test_data, valid_data = data_transformations(train_dir, test_dir, valid_dir)\n","\n","    train_loader, test_loader, valid_loader = create_loaders(train_data, test_data, valid_data)\n","\n","    device = torch.device(\"cuda\" if args.gpu == 'gpu' and torch.cuda.is_available() else \"cpu\")\n","\n","    model, input_size = model_initialization(args.arch)\n","    model.classifier = classifier_definition(input_size, args.hidden_units)\n","\n","    criterion = nn.NLLLoss()\n","    optimizer = optim.Adam(model.classifier.parameters(), lr=args.learning_rate)\n","\n","    model_training(model, train_loader, valid_loader, criterion, optimizer, device, args.epochs)\n","\n","    model_testing(model, test_loader, device)\n","\n","    save_model_checkpoint(model, optimizer, input_size, 102, args.epochs, args.save_dir)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMroxjOyRSNluNBz+A5c+CH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}